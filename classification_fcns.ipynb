{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import numpy.linalg as la\n",
    "import pandas as pd\n",
    "from scipy.linalg import svd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "from pipeline import generate_basic_exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('allegations_cleaned2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def get_svd(trainX,regularization_param,param=0):\n",
    "    # Calculate truncated SVD\n",
    "    U, S, V = la.svd(trainX)\n",
    "    \n",
    "    if regularization_param =='trunc':\n",
    "        # take inverse of each elemant\n",
    "        S_inv = np.reciprocal(S)\n",
    "        S_inv[param:] = 0\n",
    "    if regularization_param == 'rls':\n",
    "        S_inv = S/(np.square(S)+param)\n",
    "\n",
    "    new_S = np.zeros((U.shape[0],V.shape[0])) \n",
    "    np.fill_diagonal(new_S,S_inv)\n",
    "\n",
    "    return V.T@new_S.T@U.T\n",
    "\n",
    "    \n",
    "def key_min_val(d):\n",
    "#      a) create a list of the dict's keys and values; \n",
    "#      b) return the key with the min value\n",
    "    v=list(d.values())\n",
    "    k=list(d.keys())\n",
    "    \n",
    "    return k[v.index(min(v))]\n",
    "\n",
    "\n",
    "def rmse(y_hat, y):\n",
    "    return np.sqrt(((y_hat - y) ** 2).mean())\n",
    "\n",
    "def mse(y_hat, y):\n",
    "    return ((y_hat - y) ** 2).mean()\n",
    "\n",
    "\n",
    "\n",
    "def get_error_rate(w_hat,testX,testY,outcome_var_type):\n",
    "    y_hat = testX@w_hat\n",
    "    \n",
    "    if outcome_var_type == \"binary\":\n",
    "        y_hat_encode = [0 if x <= 0.5\n",
    "                        else 1 \n",
    "                        for x in y_hat]\n",
    "        \n",
    "    if outcome_var_type == 'three-class':    \n",
    "        y_hat_encode = [-1 if x <= -0.5 \n",
    "                      else 0 if x > -0.5 and x < 0.5 \n",
    "                      else 1\n",
    "                     for x in y_hat]\n",
    "\n",
    "    if outcome_var_type != \"binary\" & outcome_var_type != \"three-class\":\n",
    "        return mse(y_hat,testY)\n",
    "\n",
    "    else:\n",
    "        equal = np.sum(y_hat_encode == testY)\n",
    "        error_rate = (len(testY)-equal)/len(testY)\n",
    "\n",
    "        return error_rate\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(df,regularization_param,outcome_var,outcome_var_type=\"binary\"):\n",
    "    outer_error_rates = list()\n",
    "\n",
    "    outcomeList = ['outcome','demotion']\n",
    "    randomState = 1 \n",
    "    for i in range(50):\n",
    "        \n",
    "        inner_error_rates = dict()\n",
    "        \n",
    "        train,test = train_test_split(df,randomState=randomState)\n",
    "        trainY = train[[outcome_var]]\n",
    "        trainX = train.drop(['officer_id','Unnamed: 0']+outcomeList,axis=1)\n",
    "        testY = test[[outcome_var]]\n",
    "        testX = test.drop(['officer_id','Unnamed: 0']+outcomeList,axis=1)\n",
    "        \n",
    "        randomState += 1\n",
    "\n",
    "        if regularization_param == \"trunc\":\n",
    "            for k in range(1,10):\n",
    "                w_hat = get_svd(trainX,\"trunc\",param=k)@trainY\n",
    "                inner_error_rates[k] = get_error_rate(w_hat,testX,testY,outcome_var_type)\n",
    "\n",
    "        if regularization_param == \"rls\":\n",
    "            for lambda_ in np. array ([0 , 0.5 , 1, 2, 4, 8, 16]):\n",
    "                w_hat = get_svd(trainX,\"rls\",param=lambda_)@trainY\n",
    "                inner_error_rates[lambda_] = get_error_rate(w_hat,testX,testY,outcome_var_type)\n",
    "\n",
    "        min_key = key_min_val(inner_error_rates)\n",
    "        outer_error_rates.append((min_key,inner_error_rates[min_key]))\n",
    "            \n",
    "    return outer_error_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group train/test by officer ID somehow? See Piazza post\n",
    "def train_test_split(df,holdOut=0.2, randomState = 1):\n",
    "    # Train, val, test split:\n",
    "    # get number of unique ids and the uniqe IDs\n",
    "    n_ID = len(df.officer_id.unique())\n",
    "    ids = pd.DataFrame(df.officer_id.unique())\n",
    "\n",
    "    # sample from IDs\n",
    "    train_index = ids.sample(round(n_ID*(1-holdOut)),random_state = randomState ).values.tolist()\n",
    "    #train_index = [item for sublist in train_index for item in sublist]\n",
    "    train_index = [x[0] for x in train_index]\n",
    "    # train data is data from any IDs that show up in train index\n",
    "    train_data = df[df.officer_id.isin(train_index)]\n",
    "    # test data is data from any IDs that don't show up in train index\n",
    "    test_data = df[~df.officer_id.isin(train_index)]\n",
    "\n",
    "    # Sanity check\n",
    "    print(\"Total Number of Unique IDs:\" , len(df.officer_id.unique()))\n",
    "    print(\"Total Number of IDs in Test Data:\" , len(test_data.officer_id.unique()))\n",
    "    print(\"Total Number of IDs in Train Data:\" , len(train_data.officer_id.unique()))\n",
    "    print(\"Do the IDs add up?\" , len(test_data.officer_id.unique()) + len(train_data.officer_id.unique())  ==  len(df.officer_id.unique()))\n",
    "    print(\"Does Test Represent 20% of the data?\", (len(test_data.officer_id.unique())/len(df.officer_id.unique())) == holdOut)\n",
    "    print(\"Test Represents X% of the data:\", (len(test_data.officer_id.unique())/len(df.officer_id.unique())))\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Unique IDs: 3996\n",
      "Total Number of IDs in Test Data: 799\n",
      "Total Number of IDs in Train Data: 3197\n",
      "Do the IDs add up? True\n",
      "Does Test Represent 20% of the data? False\n",
      "Test Represents X% of the data: 0.19994994994994994\n"
     ]
    }
   ],
   "source": [
    "trunc = get_errors(df,regularization_param=\"trunc\",outcome_var=\"outcome\",outcome_var_type=\"three-class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
